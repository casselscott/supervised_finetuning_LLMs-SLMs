{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "source": ["!pip install transformers datasets torch torchvision torchaudio accelerate evaluate tensorboard pandas numpy scikit-learn"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "i6nqcCbanM0p", "outputId": "39120d2b-f4d9-4100-f037-3c7c7f217313"}, "execution_count": 19, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n", "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n", "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n", "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n", "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n", "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n", "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n", "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n", "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n", "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n", "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n", "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n", "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n", "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n", "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n", "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n", "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n", "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n", "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n", "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n", "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n", "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n", "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n", "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n", "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n", "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n", "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n", "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n", "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n", "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n", "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n", "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n", "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n", "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n", "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n", "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n", "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n", "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n", "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n", "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n", "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n", "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n", "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n", "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n", "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n", "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n", "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n", "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n", "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n", "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n", "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"]}]}, {"cell_type": "code", "execution_count": 18, "metadata": {"id": "31xlhp-FmMoM"}, "outputs": [], "source": ["import os\n", "import random\n", "import numpy as np\n", "import pandas as pd\n", "from tqdm import tqdm\n", "from typing import Dict, List, Tuple\n", "\n", "\n"]}, {"cell_type": "code", "source": ["import torch\n", "from torch.utils.data import Dataset, DataLoader\n", "from torch.optim import AdamW\n", "from torch.nn import CrossEntropyLoss\n", "import torch.nn.functional as F\n", "\n", "from transformers import (\n", "    AutoTokenizer,\n", "    AutoModelForSequenceClassification,  # For classification tasks\n", "    AutoModelForCausalLM,                 # For generative tasks\n", "    Trainer,\n", "    TrainingArguments,\n", "    get_linear_schedule_with_warmup,\n", "    set_seed\n", ")\n", "\n", "from datasets import load_dataset, Dataset as HFDataset\n", "import evaluate\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"], "metadata": {"id": "XoTnyad3nsT9"}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": ["set_seed(42)\n", "torch.manual_seed(42)\n", "np.random.seed(42)\n", "random.seed(42)\n", "\n", "# Check if CUDA is available\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ee0e-XIZoVmo", "outputId": "e9dd32ce-b1f0-402a-eb1b-9736632e466e"}, "execution_count": 12, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Using device: cpu\n"]}]}, {"cell_type": "code", "source": ["def load_and_prepare_data(dataset_path: str = \"/content/synthetic.csv\", test_size: float = 0.2):\n", "    \"\"\"\n", "    Load and prepare dataset for training and evaluation.\n", "    \"\"\"\n", "    # Load dataset using pandas\n", "    try:\n", "        df = pd.read_csv(dataset_path)\n", "    except FileNotFoundError:\n", "        raise FileNotFoundError(f\"Dataset file not found at {dataset_path}\")\n", "\n", "    # Convert pandas DataFrame to Hugging Face Dataset\n", "    dataset = HFDataset.from_pandas(df)\n", "\n", "    # Split the dataset\n", "    # Assuming the dataset has a 'train' split by default after from_pandas\n", "    # If you need to split differently, adjust this\n", "    # For demonstration, we'll use a smaller subset\n", "    if len(dataset) > 6000: # Only sample if the dataset is large enough\n", "        dataset = dataset.shuffle(seed=42).select(range(6000))\n", "\n", "    train_test_split_dataset = dataset.train_test_split(test_size=test_size, seed=42)\n", "\n", "    train_df = train_test_split_dataset['train'].to_pandas()\n", "    test_df = train_test_split_dataset['test'].to_pandas()\n", "\n", "    # Split train into train and validation\n", "    train_df, val_df = train_test_split(train_df, test_size=test_size, random_state=42)\n", "\n", "    return train_df, val_df, test_df\n", "\n", "# Load data\n", "train_df, val_df, test_df = load_and_prepare_data()\n", "\n", "\n", "class TextDataset(Dataset):\n", "    \"\"\"Custom PyTorch Dataset for text data\"\"\"\n", "    def __init__(self, texts, labels, tokenizer, max_length):\n", "        self.texts = texts\n", "        self.labels = labels\n", "        self.tokenizer = tokenizer\n", "        self.max_length = max_length\n", "\n", "    def __len__(self):\n", "        return len(self.texts)\n", "\n", "    def __getitem__(self, idx):\n", "        text = str(self.texts[idx])\n", "        label = self.labels[idx]\n", "\n", "        encoding = self.tokenizer(\n", "            text,\n", "            max_length=self.max_length,\n", "            padding=\"max_length\",\n", "            truncation=True,\n", "            return_tensors=\"pt\"\n", "        )\n", "\n", "        return {\n", "            'input_ids': encoding['input_ids'].flatten(),\n", "            'attention_mask': encoding['attention_mask'].flatten(),\n", "            'labels': torch.tensor(label, dtype=torch.long)\n", "        }\n", "\n", "\n", "\"\"\"\n", "## Model Selection and Configuration\n", "Here we'll set up our models for both LLMs and SLMs.\n", "\"\"\"\n", "\n", "\n", "def initialize_model_and_tokenizer(model_name: str, num_labels: int = 2):\n", "    \"\"\"\n", "    Initialize model and tokenizer.\n", "    Supports both LLMs and SLMs.\n", "    \"\"\"\n", "    # List of models we'll consider as \"small\"\n", "    small_models = [\n", "        'distilbert-base-uncased',\n", "        'google/mobilebert-uncased',\n", "        'huawei-noah/TinyBERT_General_4L_312D'\n", "    ]\n", "\n", "    # Determine if this is a classification or generation task\n", "    is_classification = True  # Change this if doing generation\n", "\n", "    try:\n", "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "\n", "        # Set padding token if not set\n", "        if tokenizer.pad_token is None:\n", "            tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n", "\n", "        if is_classification:\n", "            model = AutoModelForSequenceClassification.from_pretrained(\n", "                model_name,\n", "                num_labels=num_labels\n", "            )\n", "        else:\n", "            model = AutoModelForCausalLM.from_pretrained(model_name)\n", "\n", "        model.to(device)\n", "\n", "        # Print model size\n", "        num_params = sum(p.numel() for p in model.parameters())\n", "        model_type = \"SLM\" if any(sm in model_name for sm in small_models) else \"LLM\"\n", "        print(f\"Initialized {model_type}: {model_name}\")\n", "        print(f\"Number of parameters: {num_params:,}\")\n", "\n", "        return model, tokenizer\n", "\n", "    except Exception as e:\n", "        print(f\"Error loading model {model_name}: {str(e)}\")\n", "        return None, None"], "metadata": {"id": "ktFvir2YoYno"}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": ["# LLM and SLM model\n", "llm_model_name = \"bert-base-uncased\"  # Smaller LLM for demonstration\n", "slm_model_name = \"distilbert-base-uncased\"  # SLM example\n", "\n", "# Initialize both models\n", "llm_model, llm_tokenizer = initialize_model_and_tokenizer(llm_model_name)\n", "slm_model, slm_tokenizer = initialize_model_and_tokenizer(slm_model_name)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 607, "referenced_widgets": ["2998c26b7d92466ca2ea080cc4330857", "59583932ec364ce595ec024501e753da", "228ae72e49554539b9f7fbff187bf2fc", "318fae0e2e974213b2b5431108e9d23c", "64a4d152e7e440599e66d0364ed41043", "44b21584af2d40beb366d8a977a9c959", "cf5c628f21b14f3ca74d34340b6c48db", "6cc5ff63150241768d7de20c7947d935", "97b512fe09eb4a5187d2f19ae1bc68bc", "f25e696448244ddab22d92c7eee341d0", "a0d9390b52d1439a994923166420c1a8", "e1da84580791441d8f11d0d6c0fc5b74", "7e3c6d861636406cb64d831bde033a83", "fe6daa2dc2454d51bf1bf7f1b3b0fb1c", "01eb5a3aa47e470ea297ee3321e4aee2", "1e73ede0e16d40bbb3b7715805695d67", "738f3150520345f08765057535b9f585", "0ab7fd14bcbf46d396bd9f22adffd7fe", "40ecef21c5a543b88f704f99cb7af330", "388db48eef6d47e8a2b6299798f163f0", "3a8df92be7b34fe78a9acaefeea6b4f0", "72f7fa79178a46e5b9bb32348c8321bf", "b68ce59a2d7b4f48b069c5b66c35103e", "d33457745ca947fb99db366b3a0be1b1", "f8b26da9fef34877a1fc5b601a9f9690", "2773678831fa491f80a847043cceb180", "42d3e6a18d4c44959cb8b4f5b87aef09", "2ec9f6542dd842349c142fdf62b692bc", "9ae90b75efca4c7492738c54f96d490a", "ebdb86afc12d4f6d8d6e3b19c4318c4f", "b43eff9a5c464fa990b44270f03c2713", "4ca2d7d0795a4d039b1589abcb2e7730", "ce0cc75a55804037a13a6ac460ad6b90", "8cdecfcd82d7443594e0e27e4a4de0ed", "fd9914628f4d4e109c033c45b5a729ad", "5b9e4b029d81407f9a5b28114ba45a14", "0aab15e2793e4045954fbc436a311598", "5bb87beef7e04606a00a079055c30fc5", "411ccb920a064f79a407bec18f379556", "740f8a9f82204b5595c0b0f3eadef484", "c5842005b9ef493b97fc89cbc60d3d91", "e3317adb7e5f420b9affee4089fb0697", "9ba08397b982453db3e49237b9ed91fe", "45f75d01e11a41ffbd172f8001efb308", "8dac657af897422e99ceddac09ad62c8", "8779853f13544e99846cebf9a76a4aec", "7dcaea7ef054494ea2d649b53597910c", "47529ff8284141efbbd6f3f089619f81", "aace60b2fed3434a886a373a8f582eb9", "15910313639f4e57bd8e260433417c94", "4cb9f25240da4e39b7ffdf02470f4272", "d5491891d6d24ce0a351549344eec038", "9309362fa1b7433b93e81b9a7fa892f9", "e278d9204c86495da7bb8829c13939ce", "2d6788b25d71476abc50d667b2c7f33f", "1c96d69c3f2e468bb00b57b1f2f44fba", "d6f9e6a8d1ed4d428afd178b63a1f31b", "4adf4836b3534067878a4d94ebd93c39", "5945c441a8c047c68a46065db7bb64f1", "e56d199530704bf3aec18d461807e3f7", "f86491ae71a74a359f0a43ae8861504c", "444b6aafa28744ceacf8cfb3a99b3410", "d653ad9912594266a64385a42f03e50c", "13c2c00828a04a599e3b35011ea9bd73", "c1be17f138bf47e8bd621d2cebb4866f", "910873b8708d4350b5567c1853d7860c", "72ac3cb387864a11abb46a6cac8f5103", "b31e90250cee4ea3a56c50ecef4ba9b0", "cf95df740a5048328a5857eb5dc69c13", "3c0708de7bd840a5a036566cb5d7115d", "4fa2b5e685bd4473b91823b4445740f3", "95396be4e5c943908156832550d52a09", "7d3e4bb0ea2c4ac0be96bdeea4c7e0c6", "06a732b35daf425bb205f814c960c637", "0104cb31326148f0ae9d0dc8f7130e34", "a8da4f7a2edd42d4a7fce9351c4219d0", "8661d0181b3c4ac2af30fe992e163dd6", "1714a9ab047049ea957d712110b7040a", "b98abe4fffe647c9842cb41309237cc4", "540fc7e36fdc401794875f3ef3d53806", "f87aca782f904dc29b385e759bd2f8f8", "d292d394a44b412eae9283346386dc71", "639cda257f0f473ebd5cb1aa085ad26d", "90d0cd2b775a444480a46c4a4923631e", "c5140a66cccb4066ba8571e5bcf9abec", "1bed9cad7708462499ddc222100c9741", "dc7753ba3b924694999d2587038d4cab", "1262e9d9ec654e13933fc42890177d18", "85365eb5fec94000930324450086b4d6", "8c4123fc78a64ed999362b7d6cb159c1", "109b5357e5ed41a3b93d80cae4a4da8b", "f9fd60797db44443a650e97d321966b7", "2a6c7782954b449283a20b352c0a920f", "39c966915857440f91aae0f997b61de0", "5ba49ef824c8494888d39f2906f05e44", "9365d7b56a564722ac61669a1a1d1b89", "fd8f9abc89974b52bb3c8e480449b75d", "2338c6be56fd4ccdad381d08b17af87e", "8845a6a2d44e4fad9cb9df68efa7f8d6", "4a66710255b1403989848946e1fd2ab6", "f6a80d3c92014440bc6f157163400ad0", "620646ab46a540a9b6955a7f75581643", "8b73bfdbf4514853b117af5da2550a4e", "4315cf4c58b64ed48237323caa690d3f", "29bbf611e8d248ebb357b070d217a520", "103fe3c253f4480a85588e10b1df5e5b", "eb44e29f581c48d1bf41a2effeb5f539", "d1f9938f226f4a2a91a5362cbf723bf0", "04e5bf9440724e9f8dc9390652cbb5f2", "6de2a27926d54be4817dd71849ebc915"]}, "id": "86CWZwI-sqFS", "outputId": "79dfc757-1769-475e-b93f-0f37f175d4bd"}, "execution_count": 15, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2998c26b7d92466ca2ea080cc4330857"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e1da84580791441d8f11d0d6c0fc5b74"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b68ce59a2d7b4f48b069c5b66c35103e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8cdecfcd82d7443594e0e27e4a4de0ed"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8dac657af897422e99ceddac09ad62c8"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Initialized LLM: bert-base-uncased\n", "Number of parameters: 109,483,778\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1c96d69c3f2e468bb00b57b1f2f44fba"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "72ac3cb387864a11abb46a6cac8f5103"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1714a9ab047049ea957d712110b7040a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "85365eb5fec94000930324450086b4d6"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4a66710255b1403989848946e1fd2ab6"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Initialized SLM: distilbert-base-uncased\n", "Number of parameters: 66,955,010\n"]}]}, {"cell_type": "code", "source": ["## Model Training\n", "\n", "def compute_metrics(eval_pred):\n", "    logits, labels = eval_pred\n", "    predictions = np.argmax(logits, axis=-1)\n", "\n", "    return {\n", "        'accuracy': accuracy_score(labels, predictions),\n", "        'f1': f1_score(labels, predictions, average='macro'),\n", "        'precision': precision_score(labels, predictions, average='macro'),\n", "        'recall': recall_score(labels, predictions, average='macro')\n", "    }\n", "\n", "\n", "def create_data_loaders(train_df, val_df, tokenizer, batch_size=16, max_length=128):\n", "    \"\"\"Create PyTorch DataLoaders\"\"\"\n", "    train_dataset = TextDataset(\n", "        train_df['text'].values,\n", "        train_df['label'].values,\n", "        tokenizer,\n", "        max_length\n", "    )\n", "\n", "    val_dataset = TextDataset(\n", "        val_df['text'].values,\n", "        val_df['label'].values,\n", "        tokenizer,\n", "        max_length\n", "    )\n", "\n", "    train_loader = DataLoader(\n", "        train_dataset,\n", "        batch_size=batch_size,\n", "        shuffle=True\n", "    )\n", "\n", "    val_loader = DataLoader(\n", "        val_dataset,\n", "        batch_size=batch_size,\n", "        shuffle=False\n", "    )\n", "\n", "    return train_loader, val_loader\n", "\n", "\n", "def train_model(\n", "    model,\n", "    tokenizer,\n", "    train_df,\n", "    val_df,\n", "    model_name=\"model\",\n", "    batch_size=16,\n", "    learning_rate=2e-5,\n", "    num_epochs=3,\n", "    max_length=128\n", "):\n", "    \"\"\"Train the model using PyTorch\"\"\"\n", "    # Create data loaders\n", "    train_loader, val_loader = create_data_loaders(\n", "        train_df, val_df, tokenizer, batch_size, max_length\n", "    )\n", "\n", "    # Training setup\n", "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n", "    total_steps = len(train_loader) * num_epochs\n", "    scheduler = get_linear_schedule_with_warmup(\n", "        optimizer,\n", "        num_warmup_steps=0,\n", "        num_training_steps=total_steps\n", "    )\n", "\n", "    loss_fn = CrossEntropyLoss()\n", "\n", "    # Training loop\n", "    best_val_loss = float('inf')\n", "    for epoch in range(num_epochs):\n", "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n", "        print(\"-\" * 10)\n", "\n", "        # Training phase\n", "        model.train()\n", "        train_loss = 0\n", "        for batch in tqdm(train_loader, desc=\"Training\"):\n", "            input_ids = batch['input_ids'].to(device)\n", "            attention_mask = batch['attention_mask'].to(device)\n", "            labels = batch['labels'].to(device)\n", "\n", "            optimizer.zero_grad()\n", "\n", "            outputs = model(\n", "                input_ids=input_ids,\n", "                attention_mask=attention_mask,\n", "                labels=labels\n", "            )\n", "\n", "            loss = outputs.loss\n", "            train_loss += loss.item()\n", "\n", "            loss.backward()\n", "            optimizer.step()\n", "            scheduler.step()\n", "\n", "        avg_train_loss = train_loss / len(train_loader)\n", "        print(f\"Train loss: {avg_train_loss:.4f}\")\n", "\n", "        # Validation phase\n", "        model.eval()\n", "        val_loss = 0\n", "        val_preds = []\n", "        val_labels = []\n", "\n", "        with torch.no_grad():\n", "            for batch in tqdm(val_loader, desc=\"Validation\"):\n", "                input_ids = batch['input_ids'].to(device)\n", "                attention_mask = batch['attention_mask'].to(device)\n", "                labels = batch['labels'].to(device)\n", "\n", "                outputs = model(\n", "                    input_ids=input_ids,\n", "                    attention_mask=attention_mask,\n", "                    labels=labels\n", "                )\n", "\n", "                loss = outputs.loss\n", "                val_loss += loss.item()\n", "\n", "                logits = outputs.logits\n", "                preds = torch.argmax(logits, dim=1)\n", "\n", "                val_preds.extend(preds.cpu().numpy())\n", "                val_labels.extend(labels.cpu().numpy())\n", "\n", "        avg_val_loss = val_loss / len(val_loader)\n", "        val_accuracy = accuracy_score(val_labels, val_preds)\n", "\n", "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n", "        print(f\"Validation accuracy: {val_accuracy:.4f}\")\n", "\n", "        # Save best model\n", "        if avg_val_loss < best_val_loss:\n", "            best_val_loss = avg_val_loss\n", "            torch.save(model.state_dict(), f\"best_{model_name}.pt\")\n", "            print(\"Saved best model\")\n", "\n", "    # Load best model weights\n", "    model.load_state_dict(torch.load(f\"best_{model_name}.pt\"))\n", "\n", "    return model"], "metadata": {"id": "YmKukOyTs3_p"}, "execution_count": 16, "outputs": []}, {"cell_type": "code", "source": ["## Fine-Tuning the Models\n", "\n", "# Fine-tune LLM\n", "print(\"Fine-tuning LLM...\")\n", "llm_model = train_model(\n", "    llm_model,\n", "    llm_tokenizer,\n", "    train_df,\n", "    val_df,\n", "    model_name=\"llm\",\n", "    batch_size=8,  # Smaller batch size for LLM\n", "    num_epochs=1\n", ")\n", "\n", "\n", "# Fine-tune SLM\n", "print(\"\\nFine-tuning SLM...\")\n", "slm_model = train_model(\n", "    slm_model,\n", "    slm_tokenizer,\n", "    train_df,\n", "    val_df,\n", "    model_name=\"slm\",\n", "    batch_size=16,  # Larger batch size possible for SLM\n", "    num_epochs=1   # Often SLMs can train for more epochs\n", ")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "USmNdWXttTG1", "outputId": "aa3cf12c-e186-4df0-fd9a-5ee72d914c97"}, "execution_count": 17, "outputs": [{"metadata": {"tags": null}, "name": "stdout", "output_type": "stream", "text": ["Fine-tuning LLM...\n", "\n", "Epoch 1/1\n", "----------\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 160/160 [26:20<00:00,  9.88s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Train loss: 0.1165\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Validation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [01:52<00:00,  2.81s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Validation loss: 0.0096\n", "Validation accuracy: 1.0000\n", "Saved best model\n", "\n", "Fine-tuning SLM...\n", "\n", "Epoch 1/1\n", "----------\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80/80 [12:24<00:00,  9.31s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Train loss: 0.2269\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Validation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:56<00:00,  2.82s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["Validation loss: 0.0271\n", "Validation accuracy: 1.0000\n", "Saved best model\n"]}]}, {"cell_type": "code", "source": ["## Evaluation\n", "def evaluate_model(model, tokenizer, test_df, batch_size=16, max_length=128):\n", "    \"\"\"Evaluate model on test set\"\"\"\n", "    test_dataset = TextDataset(\n", "        test_df['text'].values,\n", "        test_df['label'].values,\n", "        tokenizer,\n", "        max_length\n", "    )\n", "\n", "    test_loader = DataLoader(\n", "        test_dataset,\n", "        batch_size=batch_size,\n", "        shuffle=False\n", "    )\n", "\n", "    model.eval()\n", "    test_preds = []\n", "    test_labels = []\n", "\n", "    with torch.no_grad():\n", "        for batch in tqdm(test_loader, desc=\"Testing\"):\n", "            input_ids = batch['input_ids'].to(device)\n", "            attention_mask = batch['attention_mask'].to(device)\n", "            labels = batch['labels'].to(device)\n", "\n", "            outputs = model(\n", "                input_ids=input_ids,\n", "                attention_mask=attention_mask,\n", "                labels=labels\n", "            )\n", "\n", "            logits = outputs.logits\n", "            preds = torch.argmax(logits, dim=1)\n", "\n", "            test_preds.extend(preds.cpu().numpy())\n", "            test_labels.extend(labels.cpu().numpy())\n", "\n", "    metrics = {\n", "        'accuracy': accuracy_score(test_labels, test_preds),\n", "        'f1': f1_score(test_labels, test_preds, average='macro'),\n", "        'precision': precision_score(test_labels, test_preds, average='macro'),\n", "        'recall': recall_score(test_labels, test_preds, average='macro')\n", "    }\n", "\n", "    return metrics\n", "\n", "\n", "# Evaluate LLM\n", "print(\"Evaluating LLM...\")\n", "llm_metrics = evaluate_model(llm_model, llm_tokenizer, test_df)\n", "print(\"\\nLLM Test Metrics:\")\n", "for k, v in llm_metrics.items():\n", "    print(f\"{k}: {v:.4f}\")\n", "\n", "\n", "# Evaluate SLM\n", "print(\"\\nEvaluating SLM...\")\n", "slm_metrics = evaluate_model(slm_model, slm_tokenizer, test_df)\n", "print(\"\\nSLM Test Metrics:\")\n", "for k, v in slm_metrics.items():\n", "    print(f\"{k}: {v:.4f}\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "TzFgVyuU6ajs", "outputId": "bbd16fb3-3284-42de-c5e6-9bf5ea35b9ae"}, "execution_count": 20, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Evaluating LLM...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Testing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [03:13<00:00,  7.72s/it]\n"]}, {"output_type": "stream", "name": "stdout", "text": ["\n", "LLM Test Metrics:\n", "accuracy: 1.0000\n", "f1: 1.0000\n", "precision: 1.0000\n", "recall: 1.0000\n", "\n", "Evaluating SLM...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["Testing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [01:10<00:00,  2.83s/it]"]}, {"output_type": "stream", "name": "stdout", "text": ["\n", "SLM Test Metrics:\n", "accuracy: 1.0000\n", "f1: 1.0000\n", "precision: 1.0000\n", "recall: 1.0000\n"]}, {"output_type": "stream", "name": "stderr", "text": ["\n"]}]}, {"cell_type": "code", "source": ["def train_with_trainer(model, tokenizer, train_df, val_df, model_name=\"model\"):\n", "    \"\"\"Train using HF Trainer API\"\"\"\n", "    # Convert to HF Dataset format\n", "    train_hf = HFDataset.from_pandas(train_df)\n", "    val_hf = HFDataset.from_pandas(val_df)\n", "\n", "    # Tokenize datasets\n", "    def tokenize_function(examples):\n", "        return tokenizer(\n", "            examples[\"text\"],\n", "            padding=\"max_length\",\n", "            truncation=True,\n", "            max_length=128\n", "        )\n", "\n", "    train_dataset = train_hf.map(tokenize_function, batched=True)\n", "    val_dataset = val_hf.map(tokenize_function, batched=True)\n", "\n", "    # Training arguments\n", "    training_args = TrainingArguments(\n", "        output_dir=f\"./results_{model_name}\",\n", "        evaluation_strategy=\"epoch\",\n", "        learning_rate=2e-5,\n", "        per_device_train_batch_size=8,\n", "        per_device_eval_batch_size=8,\n", "        num_train_epochs=3,\n", "        weight_decay=0.01,\n", "        save_strategy=\"epoch\",\n", "        load_best_model_at_end=True,\n", "        metric_for_best_model=\"accuracy\",\n", "        logging_dir=f'./logs_{model_name}',\n", "        logging_steps=10,\n", "        report_to=\"tensorboard\"\n", "    )\n", "\n", "    # Initialize Trainer\n", "    trainer = Trainer(\n", "        model=model,\n", "        args=training_args,\n", "        train_dataset=train_dataset,\n", "        eval_dataset=val_dataset,\n", "        compute_metrics=compute_metrics,\n", "    )\n", "\n", "    # Train\n", "    trainer.train()\n", "\n", "    return model"], "metadata": {"id": "514gyniy6pvk"}, "execution_count": 21, "outputs": []}, {"cell_type": "code", "source": ["\"\"\"\n", "## Saving and Loading Models\n", "\"\"\"\n", "\n", "def save_model(model, tokenizer, model_dir):\n", "    \"\"\"Save model and tokenizer\"\"\"\n", "    if not os.path.exists(model_dir):\n", "        os.makedirs(model_dir)\n", "\n", "    model.save_pretrained(model_dir)\n", "    tokenizer.save_pretrained(model_dir)\n", "    print(f\"Model and tokenizer saved to {model_dir}\")\n", "\n", "def load_model(model_dir):\n", "    \"\"\"Load model and tokenizer\"\"\"\n", "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n", "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n", "    model.to(device)\n", "    return model, tokenizer\n", "\n", "\n", "# Save models\n", "save_model(llm_model, llm_tokenizer, \"fine_tuned_llm\")\n", "save_model(slm_model, slm_tokenizer, \"fine_tuned_slm\")\n", "\n", "#"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "jDo1v7Wb62k8", "outputId": "70df36cd-57f4-4e5a-dba0-b8e20acd74b1"}, "execution_count": 22, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Model and tokenizer saved to fine_tuned_llm\n", "Model and tokenizer saved to fine_tuned_slm\n"]}]}]}